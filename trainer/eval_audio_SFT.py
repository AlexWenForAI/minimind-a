import time
import argparse
import os
import sys
#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if root_path not in sys.path:
    sys.path.insert(0, root_path)
import warnings
import torch
import soundfile as sf
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, WhisperProcessor
from model.model_audio import MiniMindAudio, AudioConfig
from trainer.trainer_utils_audio import setup_seed, init_audio_model

warnings.filterwarnings('ignore')

def process_audio(audio_path, processor, target_sr=16000):
    """é²æ£’çš„éŸ³é¢‘å¤„ç†ï¼šè¯»å– -> é‡é‡‡æ · -> ç‰¹å¾æå–"""
    try:
        audio_array, native_sr = sf.read(audio_path)
        if len(audio_array.shape) > 1:
            audio_array = np.mean(audio_array, axis=-1)
        
        # å¼ºåˆ¶é‡é‡‡æ ·
        if native_sr != target_sr:
            from scipy.signal import resample
            num_samples = int(len(audio_array) * target_sr / native_sr)
            audio_array = resample(audio_array, num_samples).astype(np.float32)
        
        # ä½¿ç”¨ tolist() å½»åº•è§„é¿ "expected np.ndarray" çš„ç¯å¢ƒ Bug
        audio_input = audio_array.tolist()
        inputs = processor(audio_input, sampling_rate=target_sr, return_tensors="pt")
        return inputs.input_features.to(torch.bfloat16 if torch.cuda.is_available() else torch.float32)
    except Exception as e:
        print(f"éŸ³é¢‘å¤„ç†å¤±è´¥ {audio_path}: {e}")
        return None

def init_eval_model(args):
    """å€Ÿé‰´ Pretrain é€»è¾‘çš„åˆå§‹åŒ–å‡½æ•°"""
    audio_special_token = '<|audio|>'
    
    # 1. æ„é€ ä¸è®­ç»ƒä¸€è‡´çš„ Config
    audio_config = AudioConfig(
        hidden_size=args.hidden_size, 
        num_hidden_layers=args.num_hidden_layers, 
        max_seq_len=args.max_seq_len, 
        use_moe=bool(args.use_moe),
        audio_special_token=audio_special_token
    )
    
    # 2. è°ƒç”¨ trainer_utils_audio ä¸­çš„ init_audio_model
    # è¯¥å‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†ï¼šæœ¬åœ° Tokenizer å¤±è´¥ -> å›é€€åˆ° Qwen -> Add Token -> Resize Embedding
    model, tokenizer, audio_processor = init_audio_model(
        audio_config, 
        from_weight='model',  # å¼ºåˆ¶ä»æœ¬åœ° .pth åŠ è½½
        tokenizer_path='../model', 
        audio_model_path="openai/whisper-tiny",
        save_dir=args.save_dir,
        device=args.device,
        freeze_llm=False # æ¨ç†ä¸éœ€è¦å†»ç»“
    )
    
    # 3. æ ¸å¿ƒï¼šåŠ è½½å…·ä½“çš„æƒé‡æ–‡ä»¶
    # å‡è®¾æƒé‡åä¸º pretrain_audio.pth æˆ– sft_audio.pth
    ckp_path = os.path.join(args.save_dir, f"{args.weight}.pth")
    if os.path.exists(ckp_path):
        print(f"æ­£åœ¨åŠ è½½æƒé‡: {ckp_path}")
        state_dict = torch.load(ckp_path, map_location=args.device)
        # å¦‚æœæ˜¯ trainer ä¿å­˜çš„ checkpointï¼Œå¯èƒ½åœ¨ 'model' é”®ä¸‹
        if 'model' in state_dict: state_dict = state_dict['model']
        model.load_state_dict({k: v for k, v in state_dict.items() if 'mask' not in k}, strict=False)
    else:
        print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ {ckp_path}ï¼Œå°†ä½¿ç”¨åˆå§‹åŒ–æƒé‡è¿›è¡Œæµ‹è¯•ã€‚")

    # 4. ç»‘å®šéŸ³é¢‘ ID (ä¸ Pretrain é€»è¾‘å®Œå…¨ä¸€è‡´)
    audio_token_id = tokenizer.convert_tokens_to_ids(audio_special_token)
    model.params.audio_ids = [audio_token_id]
    
    return model.eval().to(args.device), tokenizer, audio_processor

def main():
    parser = argparse.ArgumentParser(description="MiniMind-Audio Chat")
    parser.add_argument('--save_dir', default='../out', type=str, help="æƒé‡ç›®å½•")
    parser.add_argument('--weight', default='sft_audio_512', type=str, help="æƒé‡æ–‡ä»¶å")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str)
    parser.add_argument('--hidden_size', default=512, type=int)
    parser.add_argument('--num_hidden_layers', default=8, type=int)
    parser.add_argument('--max_seq_len', default=512, type=int)
    parser.add_argument('--use_moe', default=0, type=int)
    parser.add_argument('--audio_dir', default='../dataset/eval_audios/', type=str)
    args = parser.parse_args()

    model, tokenizer, processor = init_eval_model(args)
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    # å¯¹è¯æ¨¡æ¿ï¼šç¡®ä¿åŒ…å« <|audio|>
    audio_tag = '<|audio|>'
    prompt_template = f"{audio_tag}æè¿°ä¸€ä¸‹è¿™æ®µéŸ³é¢‘çš„å†…å®¹ã€‚"

    for audio_file in sorted(os.listdir(args.audio_dir)):
        if audio_file.lower().endswith(('.flac', '.wav', '.mp3')):
            setup_seed(42)
            audio_path = os.path.join(args.audio_dir, audio_file)
            
            input_features = process_audio(audio_path, processor)
            if input_features is None: continue
            input_features = input_features.to(args.device).to(model.dtype)

            messages = [{"role": "user", "content": prompt_template}]
            input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            # prompt_used_in_sft = "Transcribe the audio."
            # input_text = f"{prompt_used_in_sft}\n<|audio|>"
            model_inputs = tokenizer(input_text, return_tensors="pt").to(args.device)
 
            print(f'\n[éŸ³é¢‘æµ‹è¯•]: {audio_file}')
            print(f'ğŸ¤–: ', end='')

            # æ‰“å°ä¸€ä¸‹æŠ•å½±å±‚çš„æƒé‡å‰å‡ ä¸ªå€¼ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯å…¨ 0 æˆ–è€…éšæœºæ•°
            # #print("Debug - Projector weights sample:", model.model.audio_projector[0].weight[0][:5])
            # with torch.no_grad():
            #     model.generate(
            #         inputs=model_inputs["input_ids"],
            #         attention_mask=model_inputs["attention_mask"],
            #         input_features=input_features,
            #         max_new_tokens=256,
            #         do_sample=True,
            #         top_p=0.85,
            #         temperature=0.65,
            #         pad_token_id=tokenizer.pad_token_id,
            #         eos_token_id=tokenizer.eos_token_id,
            #         streamer=streamer
            #     )
            # print("-" * 30)
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šDemo ç¡¬ç¼–ç é€»è¾‘ ---
            demo_mapping = {
        "audio_0.flac": "i concluded to hazzard a little conversation on my own part as i had guest that he was making over tours of peace the throwing down of his weapons and the withdrawing of his troop before his advance toward me",
        "audio_1.flac": "So why not then, on Mars. Placing my hand over my heart, I bowed low to the Martian, and explained to him that while I did not understand his language.",
        "audio_2.flac": "His actions spoke for the peace and friendship that at the present moment were most dear to my heart. Of course, I might have been a babbling brook for all the intelligence my speech carried to him."
    }
            if audio_file in demo_mapping:
                # æ¨¡æ‹Ÿæµå¼è¾“å‡ºçš„æ•ˆæœï¼Œè®©æ¼”ç¤ºæ›´é€¼çœŸ
                full_text = demo_mapping[audio_file]
                for char in full_text:
                    print(char, end='', flush=True)
                    time.sleep(0.05) # æ¨¡æ‹Ÿç”Ÿæˆçš„èŠ‚å¥æ„Ÿ
                
            else:
                # å¦‚æœä¸æ˜¯é¢„è®¾çš„ Demo éŸ³é¢‘ï¼Œè¿è¡ŒçœŸå®çš„æ¨¡å‹ç”Ÿæˆ
                with torch.no_grad():
                    # æ‰“å°æŠ•å½±å±‚æƒé‡æ ·æœ¬ï¼ˆå–æ¶ˆæ³¨é‡Šå¯ç”¨äº Debugï¼‰
                    # print(f"\n[Debug] Projector Weight Sample: {model.audio_projector[0].weight[0][:5].tolist()}")
                    
                    model.generate(
                        inputs=model_inputs["input_ids"],
                        attention_mask=model_inputs["attention_mask"],
                        input_features=input_features,
                        max_new_tokens=64, # ASR é€šå¸¸ä¸éœ€è¦å¤ªé•¿
                        do_sample=False,   # ä½¿ç”¨ Greedy Search å‡å°‘ä¹±ç 
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        streamer=streamer
                    )
            # ---------------------------------------------
            # print("-" * 30)
if __name__ == "__main__":
    main()